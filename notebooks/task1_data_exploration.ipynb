{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Exploration and Enrichment\n",
    "\n",
    "## Objective\n",
    "Understand the starter dataset and enrich it with additional data useful for the forecasting task.\n",
    "\n",
    "## Dataset Overview\n",
    "- `ethiopia_fi_unified_data.xlsx` - Main dataset with observations, events, and targets\n",
    "- `reference_codes.xlsx` - Valid values for categorical fields\n",
    "- `Additional Data Points Guide.xlsx` - Guidance for data enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Examine the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main dataset\n",
    "file_path = '../data/raw/ethiopia_fi_unified_data.xlsx'\n",
    "\n",
    "# Check available sheets\n",
    "excel_file = pd.ExcelFile(file_path)\n",
    "print(\"Available sheets:\", excel_file.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main data sheet\n",
    "df_main = pd.read_excel(file_path, sheet_name='data')\n",
    "print(f\"Main dataset shape: {df_main.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the impact_links sheet\n",
    "df_impact = pd.read_excel(file_path, sheet_name='impact_links')\n",
    "print(f\"Impact links dataset shape: {df_impact.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df_impact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference codes\n",
    "ref_codes = pd.read_excel('../data/raw/reference_codes.xlsx')\n",
    "print(f\"Reference codes shape: {ref_codes.shape}\")\n",
    "print(\"\\nReference codes:\")\n",
    "ref_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understand the Schema Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine column structure\n",
    "print(\"Main dataset columns:\")\n",
    "for i, col in enumerate(df_main.columns):\n",
    "    print(f\"{i+1:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check record types distribution\n",
    "print(\"Record type distribution:\")\n",
    "record_counts = df_main['record_type'].value_counts()\n",
    "print(record_counts)\n",
    "\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print(record_counts / len(df_main) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine observations specifically\n",
    "observations = df_main[df_main['record_type'] == 'observation']\n",
    "print(f\"Observations: {len(observations)} records\")\n",
    "print(\"\\nObservation sample:\")\n",
    "observations[['indicator_code', 'indicator', 'value_numeric', 'observation_date', 'source_name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine events\n",
    "events = df_main[df_main['record_type'] == 'event']\n",
    "print(f\"Events: {len(events)} records\")\n",
    "print(\"\\nEvent sample:\")\n",
    "events[['event_date', 'category', 'description', 'source_name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine targets\n",
    "targets = df_main[df_main['record_type'] == 'target']\n",
    "print(f\"Targets: {len(targets)} records\")\n",
    "print(\"\\nTarget sample:\")\n",
    "targets[['indicator_code', 'indicator', 'value_numeric', 'target_date']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "df_main['observation_date'] = pd.to_datetime(df_main['observation_date'], errors='coerce')\n",
    "df_main['event_date'] = pd.to_datetime(df_main['event_date'], errors='coerce')\n",
    "df_main['target_date'] = pd.to_datetime(df_main['target_date'], errors='coerce')\n",
    "\n",
    "# Temporal range of observations\n",
    "obs_dates = df_main['observation_date'].dropna()\n",
    "if len(obs_dates) > 0:\n",
    "    print(f\"Observation date range: {obs_dates.min().date()} to {obs_dates.max().date()}\")\n",
    "    print(f\"Observation span: {(obs_dates.max() - obs_dates.min()).days / 365.25:.1f} years\")\n",
    "else:\n",
    "    print(\"No valid observation dates found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal range of events\n",
    "event_dates = df_main['event_date'].dropna()\n",
    "if len(event_dates) > 0:\n",
    "    print(f\"Event date range: {event_dates.min().date()} to {event_dates.max().date()}\")\n",
    "else:\n",
    "    print(\"No valid event dates found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Indicator Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique indicators and their coverage\n",
    "indicators = df_main[df_main['record_type'] == 'observation']['indicator_code'].value_counts()\n",
    "print(f\"Total unique indicators: {len(indicators)}\")\n",
    "print(\"\\nIndicator coverage:\")\n",
    "for indicator, count in indicators.items():\n",
    "    print(f\"{indicator}: {count} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pillar distribution\n",
    "pillar_counts = df_main['pillar'].value_counts()\n",
    "print(\"Pillar distribution:\")\n",
    "print(pillar_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Impact Links Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine impact links structure\n",
    "print(\"Impact links columns:\")\n",
    "for i, col in enumerate(df_impact.columns):\n",
    "    print(f\"{i+1:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact links sample\n",
    "print(\"Impact links sample:\")\n",
    "df_impact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how events connect to indicators\n",
    "print(f\"Total impact links: {len(df_impact)}\")\n",
    "print(\"\\nImpact direction distribution:\")\n",
    "print(df_impact['impact_direction'].value_counts())\n",
    "\n",
    "print(\"\\nImpact magnitude summary:\")\n",
    "print(df_impact['impact_magnitude'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in main dataset:\")\n",
    "missing_data = df_main.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df_main)) * 100\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing %': missing_percentage\n",
    "})\n",
    "print(missing_summary[missing_summary['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check confidence levels\n",
    "confidence_dist = df_main['confidence'].value_counts()\n",
    "print(\"Confidence level distribution:\")\n",
    "print(confidence_dist)\n",
    "\n",
    "print(\"\\nConfidence by record type:\")\n",
    "confidence_by_type = pd.crosstab(df_main['record_type'], df_main['confidence'])\n",
    "print(confidence_by_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reference Codes Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine reference codes structure\n",
    "print(\"Reference codes structure:\")\n",
    "ref_codes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what categorical fields have reference codes\n",
    "if 'field_name' in ref_codes.columns:\n",
    "    print(\"Fields with reference codes:\")\n",
    "    print(ref_codes['field_name'].value_counts())\n",
    "else:\n",
    "    print(\"Reference codes columns:\", ref_codes.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"=== DATASET SUMMARY ===\")\n",
    "print(f\"Total records: {len(df_main)}\")\n",
    "print(f\"Observations: {len(observations)}\")\n",
    "print(f\"Events: {len(events)}\")\n",
    "print(f\"Targets: {len(targets)}\")\n",
    "print(f\"Impact links: {len(df_impact)}\")\n",
    "print(f\"Unique indicators: {len(indicators)}\")\n",
    "print(f\"Date range: {obs_dates.min().date()} to {obs_dates.max().date()}\")\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(\"1. Dataset structure and completeness assessment\")\n",
    "print(\"2. Temporal coverage analysis\")\n",
    "print(\"3. Indicator diversity and coverage\")\n",
    "print(\"4. Event catalog and impact relationships\")\n",
    "print(\"5. Data quality and confidence levels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
